{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# callbacks.fp16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Type an introduction of the package here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": "from fastai.gen_doc.nbdoc import *\nfrom fastai.callbacks.fp16 import * "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Global Variable Definitions:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=get_master></a>`get_master`\n(<code>layer_groups</code>:<code>Collection</code>[[<code>Module</code>](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)], <code>flat_master</code>:<code>bool</code>=`False`) -> <code>Tuple</code>[<code>List</code>[<code>List</code>[<code>Tensor</code>]], <code>List</code>[<code>List</code>[<code>Tensor</code>]]]<div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L9\">[source]</a></div>\n\n\nReturns two lists, one for the model parameters in FP16 and one for the master parameters in FP32",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(get_master)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[<code>get_master</code>](fastai.callbacks.fp16.html#get_master)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=master2model></a>`master2model`\n(<code>model_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>master_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>flat_master</code>:<code>bool</code>=`False`) -> <code>NoneType</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L43\">[source]</a></div>\n\n\nCopy master parameters to model parameters",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(master2model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[<code>master2model</code>](fastai.callbacks.fp16.html#master2model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "### <a id=MixedPrecision></a><em>class</em> `MixedPrecision`\n(<code>learn</code>:[<code>Learner</code>](fastai.basic_train.html#Learner), <code>loss_scale</code>:<code>float</code>=`512.0`, <code>flat_master</code>:<code>bool</code>=`False`) -> <code>NoneType</code> :: Inherits ([<code>Callback</code>](fastai.callback.html#Callback))<div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L55\">[source]</a></div>\n\n\nCallback that handles mixed-precision training",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[<code>MixedPrecision</code>](fastai.callbacks.fp16.html#MixedPrecision)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_backward_begin></a>`on_backward_begin`\n(<code>last_loss</code>:<code>OneEltTensor</code>, <code>kwargs</code>:<code>Any</code>) -> <code>OneEltTensor</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L89\">[source]</a></div>\n\n\nScale gradients up by `loss_scale` to prevent underflow",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision.on_backward_begin)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`MixedPrecision.on_backward_begin`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_backward_end></a>`on_backward_end`(<code>kwargs</code>:<code>Any</code>)<div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L94\">[source]</a></div>\n\n\nConvert the gradients back to FP32 and divide them by the scale.",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision.on_backward_end)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`MixedPrecision.on_backward_end`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_loss_begin></a>`on_loss_begin`\n(<code>last_output</code>:<code>Tensor</code>, <code>kwargs</code>:<code>Any</code>) -> <code>Tensor</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L85\">[source]</a></div>\n\n\nConverts half precision output to FP32 to avoid reduction overflow.",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision.on_loss_begin)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`MixedPrecision.on_loss_begin`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_step_end></a>`on_step_end`(<code>kwargs</code>:<code>Any</code>) -> <code>NoneType</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L100\">[source]</a></div>\n\n\nUpdate the params from master to model and zero grad",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision.on_step_end)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`MixedPrecision.on_step_end`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_train_begin></a>`on_train_begin`(<code>kwargs</code>:<code>Any</code>) -> <code>NoneType</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L62\">[source]</a></div>\n\n\nEnsures everything is in half precision mode",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision.on_train_begin)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`MixedPrecision.on_train_begin`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_train_end></a>`on_train_end`(<code>kwargs</code>:<code>Any</code>) -> <code>NoneType</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L79\">[source]</a></div>\n\n\nRemoves half precision transforms added at `on_train_begin`",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(MixedPrecision.on_train_end)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`MixedPrecision.on_train_end`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=model_g2master_g></a>`model_g2master_g`\n(<code>model_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>master_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>flat_master</code>:<code>bool</code>=`False`) -> <code>NoneType</code><div style=\"text-align: right\"><a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L29\">[source]</a></div>\n\n\nCopies the model gradients to the master parameters for the optimizer step",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(model_g2master_g)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[<code>model_g2master_g</code>](fastai.callbacks.fp16.html#model_g2master_g)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
